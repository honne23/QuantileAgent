import torchimport numpy as npimport gymimport torch.optim as optimimport torch.nn.functional as Ffrom torch.nn.utils import clip_grad_norm_from Network.DuelingNetwork import DuelingNetworkfrom Memory.PrioritisedReplay import PrioritisedReplayimport wandbclass DDQN:        def __init__(self,                 env: gym,                 hidden_size:int = 512,                 mem_size: int = 5000,                 batch_size: int = 32,                 gamma: float = 0.99,                 lr: float = 1e-4                 ):        self.env = env        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'        self.dqn = DuelingNetwork(hidden_size, self.env.action_space.n).to(self.device)        self.target = DuelingNetwork(hidden_size, self.env.action_space.n).to(self.device)        self.target.eval()        self.target.load_state_dict(self.dqn.state_dict())        self.optimizer = optim.Adam(params= self.dqn.parameters(), lr=lr)        self.batch_size = batch_size        self.memory = PrioritisedReplay(mem_size)        self.gamma = gamma                def update_network(self) -> float:                idx, weights, samples = self.memory.sample(self.batch_size)        weights = torch.FloatTensor(weights.astype(float)).to(self.device)                samples = np.vstack(samples)        state, action, reward, next_state, done = samples.T                state = np.vstack(state).reshape(self.batch_size, 4,84,84)         next_state = np.vstack(next_state).reshape(self.batch_size,4,84,84)        state = torch.FloatTensor(state).to(self.device)        next_state = torch.FloatTensor(next_state).to(self.device)        action = torch.LongTensor(action.astype(int).reshape(-1, 1)).to(self.device)        reward = torch.FloatTensor(reward.astype(float).reshape(-1, 1)).to(self.device)        done = torch.FloatTensor(done.astype(bool).reshape(-1, 1)).to(self.device)                curr_q_value = self.dqn(state).gather(1, action)        selections = self.dqn(next_state).argmax(1).view(-1,1)        next_q_value = self.target(next_state).gather(1,selections).detach()        mask = 1 - done        target = (reward + self.gamma * next_q_value * mask).to(self.device)                loss = F.smooth_l1_loss(curr_q_value, target, reduction="none")        self.memory.update(idx, loss.detach().cpu().numpy())                loss = (loss * weights.view(-1,1)).mean()        self.optimizer.zero_grad()        loss.backward()        clip_grad_norm_(self.dqn.parameters(), 10.0)                self.optimizer.step()                self.dqn.reset_noise()        self.target.reset_noise()        loss = loss.item()        wandb.log({"Loss": loss})        return loss    def update_beta(self, frame_idx:int, num_frames:int):        fraction = min(frame_idx / num_frames, 1.0)        beta = self.memory.beta         beta = beta + fraction * (1.0 - beta)        self.memory.beta = beta                    def select_action(self, state : np.array, ready: bool) -> int:        if ready == False:            selected_action = self.env.action_space.sample()        else:            with torch.no_grad():                state = torch.FloatTensor(state).to(self.device)                selected_action = self.dqn(state.unsqueeze(0)).cpu().argmax().item()        return selected_action        def step(self, state: np.array, ready:bool) -> tuple:        action = self.select_action(state, ready)        next_state, reward, done, _ = self.env.step(action)                    transition = [state, action, reward, next_state, done]        self.memory.store(transition)        return next_state, reward, done        def target_update(self):        self.target.load_state_dict(self.dqn.state_dict())